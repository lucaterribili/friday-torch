{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    ""
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "!pip install pytorch-lightning\n",
    "\n",
    "import os\n",
    "import glob\n",
    "import math\n",
    "import torch\n",
    "from torch import Tensor\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.nn import Transformer\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "import sentencepiece as spm\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint, EarlyStopping\n",
    "import pytorch_lightning as pl\n",
    "from sklearn.utils import shuffle\n",
    "############################## DATASET\n",
    "\n",
    "class SaturdayDataset(Dataset):\n",
    "    def __init__(self, source_sentences, target_sentences, max_len, tokenizer):\n",
    "        self.source_sentences = source_sentences\n",
    "        self.target_sentences = target_sentences\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "        self.filter_invalid_pairs()\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.source_sentences)\n",
    "\n",
    "    def pad_sequence(self, original_source, original_target):\n",
    "        # Encode the source and target sequences\n",
    "        source = original_source[:]\n",
    "        target = original_target[:]\n",
    "        # Insert BOS token at the beginning of the target sequence\n",
    "        target.insert(0, self.tokenizer.bos_id())\n",
    "        target.append(self.tokenizer.eos_id())  # Add EOS token at the end\n",
    "\n",
    "        # Calculate lengths\n",
    "        source_len = len(source)\n",
    "        target_len = len(target)\n",
    "\n",
    "        # Padding for target sequence (add padding after EOS)\n",
    "        remaining_padding_for_target = self.max_len - target_len\n",
    "        if remaining_padding_for_target > 0:\n",
    "            target = target + [self.tokenizer.pad_id()] * remaining_padding_for_target\n",
    "\n",
    "        # Padding for source sequence\n",
    "        remaining_padding_for_source = self.max_len - source_len\n",
    "        if remaining_padding_for_source > 0:\n",
    "            source = source + [self.tokenizer.pad_id()] * remaining_padding_for_source\n",
    "\n",
    "        return source, target\n",
    "\n",
    "    def filter_invalid_pairs(self):\n",
    "        valid_sources = []\n",
    "        valid_targets = []\n",
    "        for src, trg in zip(self.source_sentences, self.target_sentences):\n",
    "            padded_src, padded_trg = self.pad_sequence(src, trg)\n",
    "            # Verifica che entrambi siano validi\n",
    "            if len(padded_src) <= self.max_len and len(padded_trg) <= self.max_len:\n",
    "                valid_sources.append(src)\n",
    "                valid_targets.append(trg)\n",
    "\n",
    "        # Aggiorna le liste con i dati validi\n",
    "        self.source_sentences = valid_sources\n",
    "        self.target_sentences = valid_targets\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        src = self.source_sentences[idx]\n",
    "        trg = self.target_sentences[idx]\n",
    "        # Converti le frasi in indici e aggiungi il padding\n",
    "        src_indexes, trg_indexes = self.pad_sequence(src, trg)\n",
    "\n",
    "        return torch.tensor(src_indexes), torch.tensor(trg_indexes)\n",
    "################################# MODEL\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, d_model, num_heads) -> None:\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        assert d_model % num_heads == 0\n",
    "        self.d_model = d_model\n",
    "        self.num_heads = num_heads\n",
    "        self.d_k = self.d_model // self.num_heads\n",
    "\n",
    "        self.W_Q = nn.Linear(d_model, d_model)\n",
    "        self.W_K = nn.Linear(d_model, d_model)\n",
    "        self.W_V = nn.Linear(d_model, d_model)\n",
    "\n",
    "        self.W_out = nn.Linear(d_model, d_model)\n",
    "\n",
    "    def scaled_dot_product_attention(self, q, k, v, mask=None):\n",
    "        # q,k,v are of shape (batch_size,n_heads,seq_len,single_head_dim)\n",
    "        d_k = q.size(-1)\n",
    "\n",
    "        # score is of shape (batch_size,n_heads,seq_len,seq_len)\n",
    "        scores = torch.matmul(q, k.transpose(-2, -1)) / math.sqrt(d_k)\n",
    "\n",
    "        # Ensure mask is on the same device as q, k, v\n",
    "        if mask is not None:\n",
    "            mask = mask.to(q.device)  # Move mask to the same device as the input tensors (q)\n",
    "            scores = scores.masked_fill(mask == 0, -1e9)\n",
    "\n",
    "        scores = F.softmax(scores, dim=-1)\n",
    "        attention_weights = torch.matmul(scores, v)\n",
    "        # attention_weights is of shape (batch_size,n_heads,seq_len,single_head_dim)\n",
    "        return attention_weights\n",
    "\n",
    "    def forward(self, q, k, v, mask=None):\n",
    "        \"\"\"\n",
    "            Q |\n",
    "            K | -> scaled_dot_product_attention -> concat -> linear\n",
    "            V |\n",
    "\n",
    "        \"\"\"\n",
    "        # q,k,v are of shape (batch_size,seq_length,d_model)\n",
    "        batch_size = q.size(0)\n",
    "        # calculating linear projections\n",
    "        # reshaping to (batch_size,seq_len,n_heads,single_head_dim) -> transpose to (batch_size,n_heads,seq_len,single_head_dim)\n",
    "        q = self.W_Q(q).view(batch_size, -1, self.num_heads, self.d_k).transpose(1, 2)\n",
    "        k = self.W_K(k).view(batch_size, -1, self.num_heads, self.d_k).transpose(1, 2)\n",
    "        v = self.W_V(v).view(batch_size, -1, self.num_heads, self.d_k).transpose(1, 2)\n",
    "\n",
    "        # attention\n",
    "        attention = self.scaled_dot_product_attention(q, k, v, mask)\n",
    "        batch_size, _, seq_length, d_k = attention.size()\n",
    "        output = self.W_out(attention.transpose(1, 2).contiguous().view(batch_size, seq_length, self.d_model))\n",
    "        return output\n",
    "\n",
    "\n",
    "class PositionWiseFeedForward(nn.Module):\n",
    "    def __init__(self, d_model, d_ff, dropout=0.1) -> None:\n",
    "        super(PositionWiseFeedForward, self).__init__()\n",
    "        self.linear1 = nn.Linear(d_model, d_ff)\n",
    "        self.linear2 = nn.Linear(d_ff, d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.linear2(self.dropout(F.relu(self.linear1(x))))\n",
    "\n",
    "\n",
    "class EncoderLayer(nn.Module):\n",
    "    def __init__(self, d_model, num_heads, d_ff, dropout=0.1) -> None:\n",
    "        super(EncoderLayer, self).__init__()\n",
    "        self.self_attn = MultiHeadAttention(d_model=d_model, num_heads=num_heads)\n",
    "        self.feed_forward = PositionWiseFeedForward(d_model, d_ff, dropout)\n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "        self.dropout1 = nn.Dropout(dropout)\n",
    "        self.dropout2 = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x, mask=None):\n",
    "        attn_output = self.self_attn(x, x, x, mask)\n",
    "        x = x + self.dropout1(attn_output)\n",
    "        x = self.norm1(x)\n",
    "        ff_output = self.feed_forward(x)\n",
    "        x = x + self.dropout2(ff_output)\n",
    "        x = self.norm2(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, d_model, num_heads, d_ff, num_layers, input_vocab_size, max_len=512, dropout=0.1):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.embedding = nn.Embedding(input_vocab_size, d_model)\n",
    "        self.positional_encoding = nn.Parameter(self._get_positional_encoding(max_len, d_model), requires_grad=False)\n",
    "        self.layers = nn.ModuleList([EncoderLayer(d_model, num_heads, d_ff, dropout) for _ in range(num_layers)])\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def _get_positional_encoding(self, max_len, d_model):\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        return pe.unsqueeze(0)\n",
    "\n",
    "    def forward(self, x, mask=None):\n",
    "        x = self.embedding(x) + self.positional_encoding[:, :x.size(1), :]\n",
    "        x = self.dropout(x)\n",
    "        for layer in self.layers:\n",
    "            x = layer(x, mask)\n",
    "        return x\n",
    "\n",
    "\n",
    "class DecoderLayer(nn.Module):\n",
    "    def __init__(self, d_model, num_heads, d_ff, dropout=0.1):\n",
    "        super(DecoderLayer, self).__init__()\n",
    "        self.self_attn = MultiHeadAttention(d_model, num_heads)\n",
    "        self.cross_attn = MultiHeadAttention(d_model, num_heads)\n",
    "        self.feed_forward = PositionWiseFeedForward(d_model, d_ff, dropout)\n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "        self.norm3 = nn.LayerNorm(d_model)\n",
    "        self.dropout1 = nn.Dropout(dropout)\n",
    "        self.dropout2 = nn.Dropout(dropout)\n",
    "        self.dropout3 = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x, enc_output, src_mask=None, tgt_mask=None):\n",
    "        attn_output = self.self_attn(x, x, x, tgt_mask)\n",
    "        x = x + self.dropout1(attn_output)\n",
    "        x = self.norm1(x)\n",
    "        attn_output = self.cross_attn(x, enc_output, enc_output, src_mask)\n",
    "        x = x + self.dropout2(attn_output)\n",
    "        x = self.norm2(x)\n",
    "        ff_output = self.feed_forward(x)\n",
    "        x = x + self.dropout3(ff_output)\n",
    "        x = self.norm3(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, d_model, num_heads, d_ff, num_layers, output_vocab_size, max_len=512, dropout=0.1):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.embedding = nn.Embedding(output_vocab_size, d_model)\n",
    "        self.positional_encoding = nn.Parameter(self._get_positional_encoding(max_len, d_model), requires_grad=False)\n",
    "        self.layers = nn.ModuleList([DecoderLayer(d_model, num_heads, d_ff, dropout) for _ in range(num_layers)])\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.fc_out = nn.Linear(d_model, output_vocab_size)\n",
    "\n",
    "    def _get_positional_encoding(self, max_len, d_model):\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        return pe.unsqueeze(0)\n",
    "\n",
    "    def forward(self, x, enc_output, src_mask=None, tgt_mask=None):\n",
    "        x = self.embedding(x) + self.positional_encoding[:, :x.size(1), :]\n",
    "        x = self.dropout(x)\n",
    "        for layer in self.layers:\n",
    "            x = layer(x, enc_output, src_mask, tgt_mask)\n",
    "        x = self.fc_out(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class FridayTransformer(nn.Module):\n",
    "    def __init__(self, embed_dim, src_vocab_size, target_vocab_size, num_layers=6, d_ff=2048, n_heads=8) -> None:\n",
    "        super(FridayTransformer, self).__init__()\n",
    "        self.target_vocab_size = target_vocab_size\n",
    "        self.encoder = Encoder(embed_dim, n_heads, d_ff, num_layers, src_vocab_size)\n",
    "        self.decoder = Decoder(embed_dim, n_heads, d_ff, num_layers, target_vocab_size)\n",
    "        self.num_heads = n_heads\n",
    "\n",
    "    def generate_mask(self, src, trg):\n",
    "        src_mask = (src != 0).unsqueeze(1).unsqueeze(2)\n",
    "        batch_size, trg_len = trg.shape\n",
    "        trg_mask = torch.tril(torch.ones((trg_len, trg_len))).expand(batch_size, 1, trg_len, trg_len)\n",
    "        return src_mask, trg_mask\n",
    "\n",
    "    def forward(self, src, trg):\n",
    "        src_mask, trg_mask = self.generate_mask(src, trg)\n",
    "        enc_out = self.encoder(src, src_mask)\n",
    "\n",
    "        outputs = self.decoder(trg, enc_out, src_mask, trg_mask)\n",
    "        return outputs\n",
    "\n",
    "################################# LIGHTING\n",
    "class LightningTransformer(pl.LightningModule):\n",
    "    def __init__(\n",
    "            self,\n",
    "            embed_dim,\n",
    "            vocab_size,\n",
    "            num_layers,\n",
    "            n_heads,\n",
    "            learning_rate,\n",
    "            sp_model,\n",
    "            max_len,\n",
    "            train_data=None,\n",
    "            val_data=None,\n",
    "            batch_size=50\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.dataset = None\n",
    "        self.val_dataset = None\n",
    "        self.model = FridayTransformer(embed_dim=embed_dim,\n",
    "                                         src_vocab_size=vocab_size,\n",
    "                                         target_vocab_size=vocab_size,\n",
    "                                         num_layers=num_layers,\n",
    "                                         n_heads=n_heads\n",
    "                                         )\n",
    "        self.criterion = nn.CrossEntropyLoss(ignore_index=sp_model.pad_id())\n",
    "        self.learning_rate = learning_rate\n",
    "        self.sp_model = sp_model\n",
    "        self.max_len = max_len\n",
    "        self.inputs = None\n",
    "        self.targets = None\n",
    "        self.val_inputs = None\n",
    "        self.val_targets = None\n",
    "        self.train_data = train_data\n",
    "        self.val_data = val_data\n",
    "        self.batch_size = batch_size\n",
    "        self.tokenizer = sp_model\n",
    "        self.pad_id = sp_model.pad_id()\n",
    "\n",
    "    def forward(self, src, trg_input):\n",
    "        return self.model(src, trg_input)\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        src, trg = batch\n",
    "        # src e trg sono ora di forma [batch_size, sequence_length]\n",
    "\n",
    "        # Move tensors to the same device as the model\n",
    "        src = src.to(self.device)\n",
    "        trg = trg.to(self.device)\n",
    "        tgt_input = trg[:, :-1]  # Prendiamo tutto tranne l'ultimo token (l'input per il decoder)\n",
    "\n",
    "\n",
    "        # Passiamo i dati nel modello\n",
    "        logits = self(src, tgt_input)  # Forward pass\n",
    "\n",
    "        # L'output del target è senza il primo token\n",
    "        tgt_out = trg[:, 1:]\n",
    "\n",
    "        # Calcoliamo la loss\n",
    "        loss = self.criterion(logits.reshape(-1, logits.shape[-1]), tgt_out.reshape(-1))\n",
    "        self.log(\"train_loss\", loss, on_step=True, on_epoch=True, prog_bar=True)\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        with torch.no_grad():\n",
    "            src, trg = batch\n",
    "            src = src.to(self.device)\n",
    "            trg = trg.to(self.device)\n",
    "            tgt_input = trg[:, :-1]\n",
    "\n",
    "            logits = self(src, tgt_input)\n",
    "            tgt_out = trg[:, 1:]\n",
    "\n",
    "            loss = self.criterion(logits.reshape(-1, logits.shape[-1]), tgt_out.reshape(-1))\n",
    "            self.log(\"val_loss\", loss, on_step=True, on_epoch=True, prog_bar=True)\n",
    "\n",
    "            return loss\n",
    "\n",
    "\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        return torch.optim.Adam(self.parameters(), lr=self.learning_rate)\n",
    "\n",
    "    def prepare_data(self):\n",
    "        if self.train_data is None:\n",
    "            raise ValueError(\"Train Data not provided.\")\n",
    "        inputs = [pair[0] for pair in self.train_data]\n",
    "        targets = [pair[1] for pair in self.train_data]\n",
    "        # Shuffle data to ensure randomness\n",
    "        self.inputs, self.targets = shuffle(inputs, targets)\n",
    "        if self.val_data is None:\n",
    "            raise ValueError(\"Validation Data not provided.\")\n",
    "        val_inputs = [pair[0] for pair in self.val_data]\n",
    "        val_targets = [pair[1] for pair in self.val_data]\n",
    "        self.val_inputs, self.val_targets = shuffle(val_inputs, val_targets)\n",
    "\n",
    "\n",
    "    def setup(self, stage=None):\n",
    "        if stage == 'fit' or stage is None:\n",
    "            self.dataset = SaturdayDataset(self.inputs, self.targets, self.max_len, self.sp_model)\n",
    "            self.val_dataset = SaturdayDataset(self.val_inputs, self.val_targets, self.max_len, self.sp_model)\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        return DataLoader(self.dataset, batch_size=self.batch_size, shuffle=True, num_workers=5)\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        return DataLoader(self.val_dataset, batch_size=self.batch_size, shuffle=False, num_workers=5)\n",
    "########################### INIT\n",
    "tokenizer_path = '/content/drive/MyDrive/models/tokenizer/model.model'\n",
    "sp_model = spm.SentencePieceProcessor()\n",
    "sp_model.Load(tokenizer_path)\n",
    "\n",
    "# Load inputs and targets\n",
    "train_data = np.load('/content/drive/MyDrive/dataset/transformer/multitask/dataset.npy', allow_pickle=True)\n",
    "validation_data = np.load('/content/drive/MyDrive/dataset/transformer/multitask/dataset_validation.npy', allow_pickle=True)\n",
    "\n",
    "# Parametri del modello\n",
    "embed_dim = 512\n",
    "vocab_size = sp_model.vocab_size()\n",
    "num_layers = 3\n",
    "n_heads = 8\n",
    "learning_rate = 0.0001\n",
    "max_len = 350\n",
    "\n",
    "model = LightningTransformer(\n",
    "    embed_dim=embed_dim,\n",
    "    vocab_size=vocab_size,\n",
    "    num_layers=num_layers,\n",
    "    n_heads=n_heads,\n",
    "    learning_rate=learning_rate,\n",
    "    sp_model=sp_model,\n",
    "    max_len=max_len,\n",
    "    train_data=train_data,\n",
    "    val_data=validation_data,\n",
    "    batch_size=25\n",
    ")\n",
    "\n",
    "model_dir = os.path.join(\"/content/drive/MyDrive/models/transformer/multitask/\", \"multitask.ckpt\")\n",
    "\n",
    "if os.path.exists(model_dir):  # Controllo corretto\n",
    "    try:\n",
    "        pretrained = torch.load(model_dir, map_location=\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        model.load_state_dict(pretrained[\"state_dict\"], strict=False)\n",
    "        print(\"Il modello è stato caricato correttamente.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Errore nel caricare il modello: {e}\")\n",
    "else:\n",
    "    print(\"Il file del modello non esiste nella posizione specificata.\")\n",
    "\n",
    "checkpoint_dir = \"/content/drive/MyDrive/models/transformer/multitask/checkpoints/\"\n",
    "\n",
    "# Configurazione dei callback\n",
    "checkpoint_callback = ModelCheckpoint(\n",
    "    monitor=\"val_loss\",  # Monitorare la perdita\n",
    "    dirpath=checkpoint_dir,\n",
    "    filename=\"transformer-{epoch:02d}-{val_loss:.2f}\",  # Nome file\n",
    "    save_top_k=10,  # Salva tutti i checkpoint\n",
    "    every_n_epochs=1  # Salvataggio ad ogni epoca\n",
    ")\n",
    "\n",
    "early_stopping_callback = EarlyStopping(\n",
    "    monitor=\"val_loss\",\n",
    "    patience=10,\n",
    "    verbose=True,\n",
    "    mode=\"min\"\n",
    ")\n",
    "\n",
    "trainer = pl.Trainer(\n",
    "    max_epochs=20,\n",
    "    accelerator='gpu',\n",
    "    callbacks=[checkpoint_callback, early_stopping_callback]\n",
    ")\n",
    "\n",
    "# Trova tutti i file .ckpt nella cartella\n",
    "checkpoint_files = glob.glob(os.path.join(checkpoint_dir, \"*.ckpt\"))\n",
    "\n",
    "if checkpoint_files:\n",
    "    # Ordina i file per data di modifica (il più recente per ultimo)\n",
    "    latest_checkpoint = max(checkpoint_files, key=os.path.getmtime)\n",
    "    print(f\"Checkpoint trovato: Riprendo l'allenamento da {latest_checkpoint}\")\n",
    "    trainer.fit(model, ckpt_path=latest_checkpoint)\n",
    "else:\n",
    "    print(\"Checkpoint non trovato: Parto da zero.\")\n",
    "    trainer.fit(model)"
   ],
   "id": "7902c74aae5376e8"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
